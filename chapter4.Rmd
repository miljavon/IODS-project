---
title: "chapter4"
author: "Milja von Lerber"
date: "15 helmikuuta 2017"
output: html_document
---

#Chapter 4

In this chapter I will learn about LDA and clustering. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Accessing the Boston-dataset and eploring the structure of the data.

```{r}
library(MASS)
data("Boston")
str(Boston)
```

The data has 506 observations of 14 variables.
Here I will show a brief graphical overview and show some short summaries of the variables

```{r, echo = FALSE}
library(corrplot)
library(tidyr)
```

```{r}
summary(Boston)
cor_matrix<-cor(Boston) %>% round(digits = 2)
corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
```
Now we can see the summaries of the variables and a statistical overview. 
From the variable crime we can see, that most of the people don't commit crime. It is a small precentage, that commits almost all of the crime in Boston. Crime seems to be positively related to index of accessibility to radial highways and full-value property-tax rate per \$10,000.

The average number of rooms per dwelling seems to be around six and minimum over 3. The variable is quite evenly distributed, and it is negatively related to lower status of the population and positively related to median value of homes.

Age is negatively related to Boston employment centers. The variable age seems to be quite strange though: the majority of data comes from people that are around 70 years old.The old people seem to live in areas with a high nitrogen concentration. 

Next I will scale the data and print out the summaries. 

```{r}
boston_scaled <- scale(Boston)
summary(boston_scaled)
```
Now we can see the scaled variables. It can be seen how the data is now more clearly distributed and the variables can be compared with each other. 

Here I create a new crime variable from the scaled dataset and divide the data to test and train sets, in which there are 80% of the data. 

```{r}
boston_scaled<- scale(Boston)
boston_scaled<-as.data.frame(boston_scaled)
scaled_crim <- boston_scaled$crim
bins <- quantile(scaled_crim)
crime <- cut(scaled_crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
boston_scaled$crime <- crime
n <- nrow(boston_scaled)
ind <- sample(n, size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
```

Here I will do linear discriminant analysis to observe the data. The target variable will be crime and all the other variables will act as predictors. 

```{r}
library(MASS)
lda.fit <- lda(crime ~ ., data = train)
lda.fit
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2)
lda.arrows(lda.fit, myscale = 1)
```

Saving the crime categories from the test set and removing them from the test dataset:

```{r}
testcrime <- test$crime
test <- dplyr::select(test, -crime)

```

Predicting the classes with the LDA model on the test data and cross tabulating the results:
```{r}
lda.pred <- predict(lda.fit, newdata = test)
table(correct = testcrime, predicted = lda.pred$class)
```

The results seem to be quite good: at least when it comes to low crime rates the model was most of the time able to notice that. Especially when the crime rates were high the model was able to predict it correctly. 

Reloading the Boston dataset and standardizing the dataset: 
```{r}
library(MASS)
data(Boston)
sandard_boston <- scale(Boston)
```

Calculating the distances between the observations, and running k-means algorithm.
```{r}
dist_eu <- dist(Boston)
km <-kmeans(dist_eu, centers = 4)
pairs(sandard_boston, col = km$cluster)
```

Now I'll se what the optimal number of clusters is: 
```{r}
set.seed(123)
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})
plot(1:k_max, twcss, type='b')
```

 
It seems that the optimal number of clusters is that of 2. 
```{r}
library(ggplot2)
library(GGally)
dist_eu <- dist(Boston)
km <-kmeans(dist_eu, centers = 2)
pairs(sandard_boston, col = km$cluster)
```
Here we see the clusters. I hoped I could somehow make the plot bigger. We can still see how there are two groups in everything: for example in criminality, the amount of people who commit crime is very small. 

